"""
æçŒ«ç›´æ’­åŠ©æ‰‹ - æƒ…æ„Ÿåˆ†ææ¨¡å—
è´Ÿè´£åˆ†æè¯„è®ºä¸­çš„æƒ…æ„Ÿå€¾å‘
"""

import re
from typing import Dict, List, Any, Tuple
from snownlp import SnowNLP
from collections import defaultdict
import jieba
from server.utils.logger import LoggerMixin


class SentimentAnalyzer(LoggerMixin):
    """æƒ…æ„Ÿåˆ†æå™¨"""
    
    def __init__(self, config=None):
        self.config = config or {}
        self.emotion_keywords = self._load_emotion_keywords()
        self.logger.info("æƒ…æ„Ÿåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_emotion_keywords(self) -> Dict[str, List[str]]:
        """åŠ è½½æƒ…æ„Ÿå…³é”®è¯è¯å…¸"""
        return {
            'positive': [
                'å¥½', 'æ£’', 'èµ', 'å–œæ¬¢', 'çˆ±', 'ä¼˜ç§€', 'å®Œç¾', 'æ»¡æ„', 'å¼€å¿ƒ', 'é«˜å…´',
                'ä¸é”™', 'å¯ä»¥', 'æ”¯æŒ', 'æ¨è', 'å€¼å¾—', 'æƒŠå–œ', 'æ„ŸåŠ¨', 'æ¸©æš–', 'èˆ’é€‚',
                'ç¾ä¸½', 'æ¼‚äº®', 'å¸…æ°”', 'å¯çˆ±', 'èŒ', 'å‰å®³', 'ç‰›', 'å¼º', 'ä¸“ä¸š',
                '666', 'èµ', 'ğŸ‘', 'â¤ï¸', 'ğŸ˜Š', 'ğŸ˜„', 'ğŸ˜', 'ğŸ‰', 'ğŸ‘'
            ],
            'negative': [
                'å·®', 'çƒ‚', 'åƒåœ¾', 'è®¨åŒ', 'æ¨', 'å¤±æœ›', 'ç³Ÿç³•', 'ä¸æ»¡', 'ç”Ÿæ°”', 'æ„¤æ€’',
                'ä¸‘', 'éš¾çœ‹', 'æ¶å¿ƒ', 'åæ„Ÿ', 'å‡', 'éª—', 'å‘', 'è´µ', 'ä¸å€¼', 'åæ‚”',
                'æ…¢', 'å¡', 'å', 'æ•…éšœ', 'é—®é¢˜', 'ç¼ºé™·', 'ä¸è¶³', 'ç¼ºç‚¹', 'æ¯›ç—…',
                'ğŸ‘', 'ğŸ˜¤', 'ğŸ˜¡', 'ğŸ˜­', 'ğŸ’”', 'ğŸ’©', 'ğŸ’€'
            ],
            'neutral': [
                'ä¸€èˆ¬', 'è¿˜è¡Œ', 'æ™®é€š', 'å¹³å¸¸', 'æ­£å¸¸', 'å¯ä»¥', 'è¡Œå§', 'å‡‘åˆ',
                'äº†è§£', 'çŸ¥é“', 'è¯·é—®', 'å’¨è¯¢', 'é—®', 'æƒ³', 'è€ƒè™‘', 'ç ”ç©¶',
                'å¯¹æ¯”', 'å‚è€ƒ', 'çœ‹çœ‹', 'è¯•è¯•', 'ä½“éªŒ', 'æ„Ÿå—'
            ],
            'excited': [
                'å“‡', 'å¤ªæ£’äº†', 'amazing', 'æƒŠè‰³', 'éœ‡æ’¼', 'ç»äº†', 'ç‰›é€¼', 'å‰å®³äº†',
                'æˆ‘çš„å¤©', 'å¤©å‘', 'å§æ§½', 'wtf', 'awesome', 'great', 'excellent',
                'ğŸ”¥', 'ğŸ’¥', 'ğŸ¤©', 'ğŸ¤¯', 'ğŸ˜±', 'ğŸŠ'
            ],
            'curious': [
                'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'å—', 'å‘¢', 'å§', 'å•Š',
                'è¯·æ•™', 'è¯·é—®', 'å’¨è¯¢', 'é—®', 'æ±‚', 'æƒ³è¦', 'éœ€è¦', 'äº†è§£',
                'ï¼Ÿ', '?', 'ğŸ¤”', 'â“', 'â”'
            ],
            'supportive': [
                'æ”¯æŒ', 'åŠ æ²¹', 'ç»§ç»­', 'åšæŒ', 'å…³æ³¨äº†', 'å…³æ³¨', 'è®¢é˜…', 'ç²‰',
                'æŒº', 'é¡¶', 'åŠ›æŒº', 'åŠ›é¡¶', 'å¿…é¡»', 'ä¸€å®š', 'è‚¯å®š', 'å½“ç„¶',
                'ğŸ‘', 'ğŸ’ª', 'ğŸ”¥', 'â¤ï¸', 'ğŸŒŸ'
            ]
        }
    
    def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """åˆ†æå•æ¡æ–‡æœ¬çš„æƒ…æ„Ÿ"""
        if not text:
            return {
                'sentiment': 'neutral',
                'confidence': 0.5,
                'emotions': {},
                'keywords': []
            }
        
        # ä½¿ç”¨SnowNLPè¿›è¡ŒåŸºç¡€æƒ…æ„Ÿåˆ†æ
        snow_score = SnowNLP(text).sentiments
        
        # åŸºäºå…³é”®è¯çš„æƒ…æ„Ÿåˆ†æ
        keyword_scores = self._analyze_by_keywords(text)
        
        # ç»¼åˆè¯„åˆ†
        combined_score = self._combine_scores(snow_score, keyword_scores)
        
        # ç¡®å®šä¸»è¦æƒ…æ„Ÿ
        sentiment = self._determine_sentiment(combined_score)
        
        # æå–æƒ…æ„Ÿå…³é”®è¯
        emotions, keywords = self._extract_emotion_keywords(text)
        
        return {
            'sentiment': sentiment,
            'confidence': combined_score,
            'emotions': emotions,
            'keywords': keywords,
            'snow_score': snow_score,
            'keyword_score': keyword_scores
        }
    
    def _analyze_by_keywords(self, text: str) -> Dict[str, float]:
        """åŸºäºå…³é”®è¯çš„æƒ…æ„Ÿåˆ†æ"""
        scores = defaultdict(float)
        total_matches = 0
        
        # åˆ†è¯
        words = jieba.lcut(text.lower())
        
        # ç»Ÿè®¡å„ç±»æƒ…æ„Ÿå…³é”®è¯
        for emotion, keywords in self.emotion_keywords.items():
            matches = 0
            for word in words:
                if word in keywords:
                    matches += 1
                    total_matches += 1
            scores[emotion] = matches
        
        # å½’ä¸€åŒ–å¤„ç†
        if total_matches > 0:
            for emotion in scores:
                scores[emotion] = scores[emotion] / total_matches
        
        return dict(scores)
    
    def _combine_scores(self, snow_score: float, keyword_scores: Dict[str, float]) -> float:
        """ç»¼åˆä¸åŒæ–¹æ³•çš„è¯„åˆ†"""
        # SnowNLPè¯„åˆ†æƒé‡0.6ï¼Œå…³é”®è¯è¯„åˆ†æƒé‡0.4
        keyword_positive = keyword_scores.get('positive', 0) + keyword_scores.get('excited', 0) + keyword_scores.get('supportive', 0)
        keyword_negative = keyword_scores.get('negative', 0)
        keyword_neutral = keyword_scores.get('neutral', 0) + keyword_scores.get('curious', 0)
        
        # å°†å…³é”®è¯è¯„åˆ†è½¬æ¢ä¸ºæƒ…æ„Ÿå€¾å‘å€¼(-1åˆ°1)
        keyword_sentiment = keyword_positive - keyword_negative
        
        # ç»¼åˆè¯„åˆ†(0åˆ°1)
        combined = 0.6 * snow_score + 0.4 * ((keyword_sentiment + 1) / 2)
        
        return combined
    
    def _determine_sentiment(self, score: float) -> str:
        """æ ¹æ®è¯„åˆ†ç¡®å®šæƒ…æ„Ÿç±»åˆ«"""
        if score >= 0.7:
            return 'positive'
        elif score >= 0.6:
            return 'excited'
        elif score >= 0.4:
            return 'neutral'
        elif score >= 0.3:
            return 'curious'
        elif score >= 0.2:
            return 'supportive'
        else:
            return 'negative'
    
    def _extract_emotion_keywords(self, text: str) -> Tuple[Dict[str, int], List[str]]:
        """æå–æƒ…æ„Ÿå…³é”®è¯"""
        emotions = defaultdict(int)
        keywords = []
        
        # åˆ†è¯
        words = jieba.lcut(text.lower())
        
        # ç»Ÿè®¡æƒ…æ„Ÿå…³é”®è¯
        for word in words:
            for emotion, emotion_keywords in self.emotion_keywords.items():
                if word in emotion_keywords:
                    emotions[emotion] += 1
                    if word not in keywords:
                        keywords.append(word)
        
        return dict(emotions), keywords
    
    def analyze_comments(self, comments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æè¯„è®ºåˆ—è¡¨çš„æƒ…æ„Ÿåˆ†å¸ƒ"""
        if not comments:
            return {
                'overall_sentiment': 'neutral',
                'sentiment_distribution': {},
                'average_confidence': 0.0,
                'total_comments': 0,
                'emotion_stats': {}
            }
        
        sentiment_counts = defaultdict(int)
        emotion_stats = defaultdict(int)
        total_confidence = 0.0
        detailed_results = []
        
        for comment in comments:
            content = comment.get('content', '')
            result = self.analyze_sentiment(content)
            detailed_results.append(result)
            
            sentiment_counts[result['sentiment']] += 1
            total_confidence += result['confidence']
            
            # ç»Ÿè®¡æƒ…æ„Ÿå…³é”®è¯
            for emotion, count in result['emotions'].items():
                emotion_stats[emotion] += count
        
        total_comments = len(comments)
        average_confidence = total_confidence / total_comments if total_comments > 0 else 0.0
        
        # è®¡ç®—æƒ…æ„Ÿåˆ†å¸ƒç™¾åˆ†æ¯”
        sentiment_distribution = {}
        for sentiment, count in sentiment_counts.items():
            sentiment_distribution[sentiment] = {
                'count': count,
                'percentage': round((count / total_comments) * 100, 2)
            }
        
        # ç¡®å®šæ•´ä½“æƒ…æ„Ÿå€¾å‘
        overall_sentiment = max(sentiment_counts.items(), key=lambda x: x[1])[0] if sentiment_counts else 'neutral'
        
        return {
            'overall_sentiment': overall_sentiment,
            'sentiment_distribution': sentiment_distribution,
            'average_confidence': round(average_confidence, 4),
            'total_comments': total_comments,
            'emotion_stats': dict(emotion_stats),
            'detailed_results': detailed_results
        }
    
    def get_sentiment_trend(self, comments: List[Dict[str, Any]], time_windows: int = 5) -> List[Dict[str, Any]]:
        """è·å–æƒ…æ„Ÿè¶‹åŠ¿"""
        if not comments:
            return []
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_comments = sorted(comments, key=lambda x: x.get('timestamp', 0))
        
        # åˆ†å‰²æ—¶é—´æ®µ
        total_comments = len(sorted_comments)
        window_size = max(1, total_comments // time_windows)
        
        trend_data = []
        for i in range(0, total_comments, window_size):
            window_comments = sorted_comments[i:i+window_size]
            if not window_comments:
                continue
            
            analysis = self.analyze_comments(window_comments)
            latest_timestamp = window_comments[-1].get('timestamp', 0)
            
            trend_data.append({
                'timestamp': latest_timestamp,
                'sentiment': analysis['overall_sentiment'],
                'distribution': analysis['sentiment_distribution'],
                'confidence': analysis['average_confidence'],
                'comment_count': len(window_comments)
            })
        
        return trend_data