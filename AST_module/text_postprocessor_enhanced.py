#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›´æ’­è¯æ±‡åå¤„ç†å™¨
ä¸ºæƒ…æ„Ÿåšä¸»è¯­éŸ³è¯†åˆ«åœºæ™¯ä¼˜åŒ–çš„æ–‡æœ¬åå¤„ç†ç»„ä»¶

ä¸»è¦åŠŸèƒ½:
1. æƒ…æ„Ÿè¡¨è¾¾çº é”™
2. äº§å“åç§°æ ‡å‡†åŒ–
3. ç½‘ç»œç”¨è¯­è§„èŒƒåŒ–
4. è¯­æ³•é”™è¯¯ä¿®æ­£
5. æ ‡ç‚¹ç¬¦å·ä¼˜åŒ–
"""

import re
import logging
from typing import List, Dict, Optional, Tuple, Any, Set
from dataclasses import dataclass, field
from collections import defaultdict
import difflib
from enhanced_transcription_result import (
    ProcessingType,
    WordAnalysis,
    EmotionType
)

# åå¤„ç†å¸¸é‡
MAX_CORRECTION_DISTANCE = 2    # æœ€å¤§ç¼–è¾‘è·ç¦»
MIN_CONFIDENCE_FOR_CORRECTION = 0.3  # çº é”™æœ€å°ç½®ä¿¡åº¦
PRODUCT_BRAND_WEIGHT = 1.5     # å“ç‰Œåç§°æƒé‡

@dataclass
class CorrectionRule:
    """çº é”™è§„åˆ™"""
    pattern: str                # åŒ¹é…æ¨¡å¼ï¼ˆæ­£åˆ™æˆ–å­—ç¬¦ä¸²ï¼‰
    replacement: str           # æ›¿æ¢å†…å®¹
    rule_type: str            # è§„åˆ™ç±»å‹
    confidence: float         # è§„åˆ™ç½®ä¿¡åº¦
    conditions: List[str] = field(default_factory=list)  # åº”ç”¨æ¡ä»¶

@dataclass
class PostProcessingResult:
    """åå¤„ç†ç»“æœ"""
    original_text: str         # åŸå§‹æ–‡æœ¬
    processed_text: str        # å¤„ç†åæ–‡æœ¬
    applied_corrections: List[Dict[str, Any]]  # åº”ç”¨çš„çº é”™
    processing_confidence: float  # å¤„ç†ç½®ä¿¡åº¦
    processing_types: List[ProcessingType]  # å¤„ç†ç±»å‹

class EmotionExpressionCorrector:
    """æƒ…æ„Ÿè¡¨è¾¾çº é”™å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # æƒ…æ„Ÿè¡¨è¾¾çº é”™è¯å…¸
        self.emotion_corrections = {
            # å…´å¥‹è¡¨è¾¾çº é”™
            "å…´å¥‹ç±»": {
                "å¤ªæ£’": "å¤ªæ£’äº†", "ç»": "ç»äº†", "çˆ±": "çˆ±äº†", 
                "å“‡å¡": "å“‡", "å‰å®³": "å‰å®³äº†", "ç‰›": "ç‰›é€¼",
                "é¡¶": "é¡¶å‘±å‘±", "èµ": "ç‚¹èµ", "å¥½è€¶": "å¥½æ£’"
            },
            
            # å–œçˆ±è¡¨è¾¾çº é”™
            "å–œçˆ±ç±»": {
                "å–œæ¬¢æ­»": "è¶…çº§å–œæ¬¢", "çˆ±æ­»": "ç‰¹åˆ«çˆ±", "è¿·": "è¿·æ‹",
                "ä¸­æ¯’": "ä¸­æ„", "å…¥å‘": "å–œæ¬¢ä¸Š", "ç§è‰": "æ¨è",
                "å¿ƒåŠ¨": "å¿ƒåŠ¨äº†", "æ²¦é™·": "æ²¦é™·äº†"
            },
            
            # æƒŠå¹è¡¨è¾¾çº é”™
            "æƒŠå¹ç±»": {
                "æˆ‘å»": "å“‡", "å§æ§½": "å“‡å¡", "æˆ‘é ": "å¤©å“ª",
                "æˆ‘æ»´": "æˆ‘çš„", "å¦ˆå‘€": "å¤©å•Š", "ä¹–ä¹–": "ä¹–ä¹–"
            },
            
            # èµç¾è¡¨è¾¾çº é”™
            "èµç¾ç±»": {
                "ç¥ä»™": "ç¥ä»™çº§åˆ«", "å®è—": "å®è—äº§å“", "æ— æ•Œ": "æ— æ•Œå¥½ç”¨",
                "å®Œç¾": "å¤ªå®Œç¾", "æƒŠè‰³": "å¤ªæƒŠè‰³", "æ²»æ„ˆ": "å¾ˆæ²»æ„ˆ"
            }
        }
        
        # è¯­æ°”åŠ©è¯çº é”™
        self.modal_particles = {
            "å•Š": ["å‘€", "å“å‘€", "å“‡"],
            "å‘¢": ["å˜", "å’§"],
            "çš„": ["æ»´", "åœ°"],
            "äº†": ["å•¦", "è¾£"],
            "å§": ["å­", "å·´"]
        }
        
        # æ„å»ºçº é”™è§„åˆ™
        self.correction_rules = self._build_correction_rules()
    
    def _build_correction_rules(self) -> List[CorrectionRule]:
        """æ„å»ºçº é”™è§„åˆ™"""
        rules = []
        
        # æƒ…æ„Ÿè¯æ±‡çº é”™è§„åˆ™
        for category, corrections in self.emotion_corrections.items():
            for wrong, correct in corrections.items():
                rule = CorrectionRule(
                    pattern=wrong,
                    replacement=correct,
                    rule_type="emotion_correction",
                    confidence=0.8,
                    conditions=[f"emotion_context:{category}"]
                )
                rules.append(rule)
        
        # è¯­æ°”åŠ©è¯çº é”™è§„åˆ™
        for correct, variants in self.modal_particles.items():
            for variant in variants:
                rule = CorrectionRule(
                    pattern=variant,
                    replacement=correct,
                    rule_type="modal_particle_correction",
                    confidence=0.7
                )
                rules.append(rule)
        
        return rules
    
    def correct_emotion_expressions(self, text: str, 
                                  emotion_type: Optional[EmotionType] = None) -> Tuple[str, List[Dict[str, Any]]]:
        """çº æ­£æƒ…æ„Ÿè¡¨è¾¾
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            emotion_type: æƒ…æ„Ÿç±»å‹æç¤º
            
        Returns:
            (çº æ­£åæ–‡æœ¬, åº”ç”¨çš„çº é”™åˆ—è¡¨)
        """
        try:
            corrected_text = text
            applied_corrections = []
            
            # æ ¹æ®æƒ…æ„Ÿç±»å‹é€‰æ‹©ç›¸å…³è§„åˆ™
            relevant_rules = self._filter_rules_by_emotion(emotion_type)
            
            for rule in relevant_rules:
                if rule.pattern in corrected_text:
                    # åº”ç”¨çº é”™
                    new_text = corrected_text.replace(rule.pattern, rule.replacement)
                    
                    if new_text != corrected_text:
                        correction = {
                            "type": rule.rule_type,
                            "original": rule.pattern,
                            "corrected": rule.replacement,
                            "confidence": rule.confidence,
                            "position": corrected_text.find(rule.pattern)
                        }
                        applied_corrections.append(correction)
                        corrected_text = new_text
            
            return corrected_text, applied_corrections
            
        except Exception as e:
            self.logger.error(f"æƒ…æ„Ÿè¡¨è¾¾çº é”™å¤±è´¥: {e}")
            return text, []
    
    def _filter_rules_by_emotion(self, emotion_type: Optional[EmotionType]) -> List[CorrectionRule]:
        """æ ¹æ®æƒ…æ„Ÿç±»å‹è¿‡æ»¤è§„åˆ™"""
        if emotion_type is None:
            return self.correction_rules
        
        # æ ¹æ®æƒ…æ„Ÿç±»å‹é€‰æ‹©ç›¸å…³è§„åˆ™
        emotion_category_map = {
            EmotionType.EXCITED: ["å…´å¥‹ç±»", "æƒŠå¹ç±»"],
            EmotionType.JOYFUL: ["å–œçˆ±ç±»", "èµç¾ç±»"],
            EmotionType.NEUTRAL: ["modal_particle_correction"]
        }
        
        relevant_categories = emotion_category_map.get(emotion_type, [])
        
        filtered_rules = []
        for rule in self.correction_rules:
            # æ£€æŸ¥è§„åˆ™æ˜¯å¦ä¸å½“å‰æƒ…æ„Ÿç›¸å…³
            is_relevant = False
            for condition in rule.conditions:
                if any(cat in condition for cat in relevant_categories):
                    is_relevant = True
                    break
            
            if is_relevant or rule.rule_type == "modal_particle_correction":
                filtered_rules.append(rule)
        
        return filtered_rules

class ProductNameStandardizer:
    """äº§å“åç§°æ ‡å‡†åŒ–å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # å“ç‰Œåç§°æ ‡å‡†åŒ–è¯å…¸
        self.brand_standardization = {
            # åŒ–å¦†å“å“ç‰Œ
            "é›…è¯—å…°é»›": ["é›…è¯—å…°ä»£", "é›…è¯—è˜­é»›", "estee lauder"],
            "å…°è”»": ["å…°å¯‡", "lancome", "è˜­è”»"],
            "è¿ªå¥¥": ["dior", "è¿ªæ¾³", "è¿ªå¥§"],
            "é¦™å¥ˆå„¿": ["chanel", "é¦™å¥ˆå°”", "é¦™å¥ˆå…’"],
            "åœ£ç½—å…°": ["ysl", "è–ç¾…è˜­", "åœ£ç½—å…°"],
            "é˜¿ç›å°¼": ["armani", "é˜¿ç‘ªå°¼"],
            "æ¬§è±é›…": ["loreal", "æ­èŠé›…", "æ¬§æ¥é›…"],
            "cpb": ["è‚Œè‚¤ä¹‹é’¥", "cle de peau"],
            "sk2": ["sk-ii", "skii", "skäºŒ"],
            "æµ·è“ä¹‹è°œ": ["la mer", "æµ·è—ä¹‹è¬"],
            
            # æœè£…å“ç‰Œ
            "å¤é©°": ["gucci", "å¤å¥‡"],
            "è·¯æ˜“å¨ç™»": ["lv", "louis vuitton", "è·¯æ˜“Â·å¨ç™»"],
            "çˆ±é©¬ä»•": ["hermes", "æ„›é¦¬ä»•"],
            "æ™®æ‹‰è¾¾": ["prada", "æ™®æ‹‰é”"],
            "å·´é»ä¸–å®¶": ["balenciaga", "å·´é»ä¸–å®¶"],
            
            # å›½äº§å“ç‰Œ
            "èŠ±è¥¿å­": ["èŠ±è¥¿å­œ", "èŠ±è¥¿ç´«"],
            "å®Œç¾æ—¥è®°": ["å®Œç¾æ—¥è¨˜", "perfect diary"],
            "è–‡è¯ºå¨œ": ["è–‡è¯ºå¨œ", "winona"],
            "è‡ªç„¶å ‚": ["è‡ªç„¶å ‚", "chando"]
        }
        
        # äº§å“ç±»å‹æ ‡å‡†åŒ–
        self.product_type_standardization = {
            "å£çº¢": ["å”‡è†", "lipstick", "lip stick", "å”‡å½©"],
            "ç²‰åº•": ["foundation", "ç²‰åº•æ¶²", "åº•å¦†"],
            "çœ¼å½±": ["eye shadow", "eyeshadow", "çœ¼å½±ç›˜"],
            "ç«æ¯›è†": ["mascara", "ç«æ¯›æ¶²"],
            "é¢è†œ": ["mask", "é¢è†œçº¸", "è´´ç‰‡é¢è†œ"],
            "ç²¾å": ["essence", "serum", "ç²¾åæ¶²"],
            "ä¹³æ¶²": ["lotion", "æ¶¦è‚¤ä¹³"],
            "é˜²æ™’": ["sunscreen", "é˜²æ™’éœœ", "é˜²æ™’ä¹³"],
            "å¸å¦†": ["makeup remover", "å¸å¦†æ°´", "å¸å¦†æ²¹"]
        }
        
        # é¢œè‰²åç§°æ ‡å‡†åŒ–
        self.color_standardization = {
            "æ­£çº¢è‰²": ["å¤§çº¢", "æ­£çº¢", "çº¢è‰²"],
            "ç«ç‘°çº¢": ["ç«çº¢", "rose", "ç«ç‘°è‰²"],
            "çŠç‘šçº¢": ["çŠç‘šè‰²", "coral", "æ©˜çº¢"],
            "è±†æ²™è‰²": ["è±†æ²™", "nude", "è£¸è‰²"],
            "å§¨å¦ˆçº¢": ["å§¨å¦ˆè‰²", "æ·±çº¢", "æš—çº¢"],
            "æ–©ç”·è‰²": ["æ–©ç”·", "é­…æƒ‘çº¢"],
            "å’¬å”‡å¦†": ["å’¬å”‡è‰²", "è‡ªç„¶çº¢"]
        }
    
    def standardize_product_names(self, text: str) -> Tuple[str, List[Dict[str, Any]]]:
        """æ ‡å‡†åŒ–äº§å“åç§°
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            (æ ‡å‡†åŒ–åæ–‡æœ¬, åº”ç”¨çš„æ ‡å‡†åŒ–åˆ—è¡¨)
        """
        try:
            standardized_text = text
            applied_standardizations = []
            
            # 1. å“ç‰Œåç§°æ ‡å‡†åŒ–
            standardized_text, brand_corrections = self._standardize_brands(standardized_text)
            applied_standardizations.extend(brand_corrections)
            
            # 2. äº§å“ç±»å‹æ ‡å‡†åŒ–
            standardized_text, type_corrections = self._standardize_product_types(standardized_text)
            applied_standardizations.extend(type_corrections)
            
            # 3. é¢œè‰²åç§°æ ‡å‡†åŒ–
            standardized_text, color_corrections = self._standardize_colors(standardized_text)
            applied_standardizations.extend(color_corrections)
            
            return standardized_text, applied_standardizations
            
        except Exception as e:
            self.logger.error(f"äº§å“åç§°æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return text, []
    
    def _standardize_brands(self, text: str) -> Tuple[str, List[Dict[str, Any]]]:
        """æ ‡å‡†åŒ–å“ç‰Œåç§°"""
        corrected_text = text
        corrections = []
        
        for standard_name, variants in self.brand_standardization.items():
            for variant in variants:
                # ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
                pattern = re.compile(re.escape(variant), re.IGNORECASE)
                matches = pattern.finditer(corrected_text)
                
                for match in matches:
                    correction = {
                        "type": "brand_standardization",
                        "original": match.group(),
                        "corrected": standard_name,
                        "confidence": 0.9,
                        "position": match.start()
                    }
                    corrections.append(correction)
                
                corrected_text = pattern.sub(standard_name, corrected_text)
        
        return corrected_text, corrections
    
    def _standardize_product_types(self, text: str) -> Tuple[str, List[Dict[str, Any]]]:
        """æ ‡å‡†åŒ–äº§å“ç±»å‹"""
        corrected_text = text
        corrections = []
        
        for standard_type, variants in self.product_type_standardization.items():
            for variant in variants:
                if variant in corrected_text:
                    correction = {
                        "type": "product_type_standardization",
                        "original": variant,
                        "corrected": standard_type,
                        "confidence": 0.85,
                        "position": corrected_text.find(variant)
                    }
                    corrections.append(correction)
                    corrected_text = corrected_text.replace(variant, standard_type)
        
        return corrected_text, corrections
    
    def _standardize_colors(self, text: str) -> Tuple[str, List[Dict[str, Any]]]:
        """æ ‡å‡†åŒ–é¢œè‰²åç§°"""
        corrected_text = text
        corrections = []
        
        for standard_color, variants in self.color_standardization.items():
            for variant in variants:
                if variant in corrected_text:
                    correction = {
                        "type": "color_standardization",
                        "original": variant,
                        "corrected": standard_color,
                        "confidence": 0.8,
                        "position": corrected_text.find(variant)
                    }
                    corrections.append(correction)
                    corrected_text = corrected_text.replace(variant, standard_color)
        
        return corrected_text, corrections

class SlangNormalizer:
    """ç½‘ç»œç”¨è¯­è§„èŒƒåŒ–å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # ç½‘ç»œç”¨è¯­è§„èŒƒåŒ–è¯å…¸
        self.slang_normalization = {
            # æµè¡Œç½‘ç»œç”¨è¯­
            "yyds": "æ°¸è¿œçš„ç¥",
            "ç»ç»å­": "ç»äº†",
            "çˆ±äº†çˆ±äº†": "å¤ªå–œæ¬¢äº†",
            "å¤ªç»äº†": "å¤ªæ£’äº†",
            "å·¨å¥½ç”¨": "éå¸¸å¥½ç”¨",
            "è´¼å¥½çœ‹": "ç‰¹åˆ«å¥½çœ‹",
            "è¶…A": "è¶…çº§æ£’",
            "æ— æ•Œäº†": "å¤ªå‰å®³äº†",
            "ç¥ä»™é¢œå€¼": "é¢œå€¼å¾ˆé«˜",
            "é¢œå€¼å¤©èŠ±æ¿": "é¢œå€¼æœ€é«˜",
            
            # è´­ç‰©ç›¸å…³ç½‘ç»œç”¨è¯­
            "ç§è‰": "æ¨è",
            "æ‹”è‰": "ä¹°äº†",
            "é•¿è‰": "æƒ³ä¹°",
            "å‰æ‰‹": "è´­ä¹°",
            "å…¥å‘": "å¼€å§‹å–œæ¬¢",
            "å®‰åˆ©": "æ¨è",
            "å›è´­": "å†æ¬¡è´­ä¹°",
            "æ— é™å›è´­": "ä¼šä¸€ç›´ä¹°",
            
            # ç¾å¦†ç›¸å…³ç½‘ç»œç”¨è¯­
            "ç´ é¢œéœœ": "æäº®é¢éœœ",
            "çŒªè‚è‰²": "æ·±çº¢è‰²",
            "æ–©ç”·è‰²": "é­…åŠ›çº¢è‰²",
            "æ­»äº¡èŠ­æ¯”ç²‰": "äº®ç²‰è‰²",
            "å§¨å¦ˆè‰²": "æ·±çº¢è‰²",
            "åƒåœŸè‰²": "æ£•è‰²ç³»",
            
            # è¯­æ°”å¼ºåŒ–è¯
            "å·¨": "éå¸¸",
            "è´¼": "ç‰¹åˆ«",
            "è¶…": "å¾ˆ",
            "ç‹‚": "éå¸¸",
            "æš´": "ç‰¹åˆ«"
        }
        
        # ç¼©å†™è¯è§„èŒƒåŒ–
        self.abbreviation_normalization = {
            "cpb": "è‚Œè‚¤ä¹‹é’¥",
            "tf": "æ±¤å§†ç¦ç‰¹",
            "ysl": "åœ£ç½—å…°",
            "ct": "å¤æ´›ç‰¹Â·è’‚å°”ä¼¯é‡Œ",
            "nars": "çº³æ–¯",
            "mac": "é­…å¯",
            "3ce": "ä¸‰ç†¹ç‰",
            "vdl": "vdl"
        }
        
        # è¡¨æƒ…ç¬¦å·å¤„ç†
        self.emoji_normalization = {
            "ğŸ˜": "å¤ªå–œæ¬¢äº†",
            "ğŸ¥°": "å¾ˆå¯çˆ±",
            "ğŸ˜˜": "ä¹ˆä¹ˆå“’",
            "ğŸ‘": "é¼“æŒ",
            "ğŸ’•": "å–œæ¬¢",
            "â¤ï¸": "çˆ±",
            "âœ¨": "",  # è£…é¥°æ€§è¡¨æƒ…åˆ é™¤
            "ğŸŒŸ": "",
            "ğŸ’–": "çˆ±å¿ƒ"
        }
    
    def normalize_slang(self, text: str) -> Tuple[str, List[Dict[str, Any]]]:
        """è§„èŒƒåŒ–ç½‘ç»œç”¨è¯­
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            (è§„èŒƒåŒ–åæ–‡æœ¬, åº”ç”¨çš„è§„èŒƒåŒ–åˆ—è¡¨)
        """
        try:
            normalized_text = text
            applied_normalizations = []
            
            # 1. ç½‘ç»œç”¨è¯­è§„èŒƒåŒ–
            normalized_text, slang_corrections = self._normalize_slang_words(normalized_text)
            applied_normalizations.extend(slang_corrections)
            
            # 2. ç¼©å†™è¯è§„èŒƒåŒ–
            normalized_text, abbrev_corrections = self._normalize_abbreviations(normalized_text)
            applied_normalizations.extend(abbrev_corrections)
            
            # 3. è¡¨æƒ…ç¬¦å·å¤„ç†
            normalized_text, emoji_corrections = self._normalize_emojis(normalized_text)
            applied_normalizations.extend(emoji_corrections)
            
            return normalized_text, applied_normalizations
            
        except Exception as e:
            self.logger.error(f"ç½‘ç»œç”¨è¯­è§„èŒƒåŒ–å¤±è´¥: {e}")
            return text, []
    
    def _normalize_slang_words(self, text: str) -> Tuple[str, List[Dict[str, Any]]]:
        """è§„èŒƒåŒ–ç½‘ç»œç”¨è¯­è¯æ±‡"""
        normalized_text = text
        corrections = []
        
        # æŒ‰è¯é•¿åº¦æ’åºï¼Œä¼˜å…ˆå¤„ç†é•¿è¯æ±‡é¿å…éƒ¨åˆ†åŒ¹é…
        sorted_slang = sorted(self.slang_normalization.items(), 
                            key=lambda x: len(x[0]), reverse=True)
        
        for slang, formal in sorted_slang:
            if slang in normalized_text:
                correction = {
                    "type": "slang_normalization",
                    "original": slang,
                    "corrected": formal,
                    "confidence": 0.75,
                    "position": normalized_text.find(slang)
                }
                corrections.append(correction)
                normalized_text = normalized_text.replace(slang, formal)
        
        return normalized_text, corrections
    
    def _normalize_abbreviations(self, text: str) -> Tuple[str, List[Dict[str, Any]]]:
        """è§„èŒƒåŒ–ç¼©å†™è¯"""
        normalized_text = text
        corrections = []
        
        for abbrev, full_name in self.abbreviation_normalization.items():
            # ä½¿ç”¨æ­£åˆ™åŒ¹é…ç‹¬ç«‹çš„ç¼©å†™è¯
            pattern = r'\b' + re.escape(abbrev) + r'\b'
            matches = re.finditer(pattern, normalized_text, re.IGNORECASE)
            
            for match in matches:
                correction = {
                    "type": "abbreviation_normalization",
                    "original": match.group(),
                    "corrected": full_name,
                    "confidence": 0.9,
                    "position": match.start()
                }
                corrections.append(correction)
            
            normalized_text = re.sub(pattern, full_name, normalized_text, flags=re.IGNORECASE)
        
        return normalized_text, corrections
    
    def _normalize_emojis(self, text: str) -> Tuple[str, List[Dict[str, Any]]]:
        """è§„èŒƒåŒ–è¡¨æƒ…ç¬¦å·"""
        normalized_text = text
        corrections = []
        
        for emoji, replacement in self.emoji_normalization.items():
            if emoji in normalized_text:
                correction = {
                    "type": "emoji_normalization",
                    "original": emoji,
                    "corrected": replacement,
                    "confidence": 0.6,
                    "position": normalized_text.find(emoji)
                }
                corrections.append(correction)
                normalized_text = normalized_text.replace(emoji, replacement)
        
        return normalized_text, corrections

class GrammarCorrector:
    """è¯­æ³•é”™è¯¯ä¿®æ­£å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # å¸¸è§è¯­æ³•é”™è¯¯æ¨¡å¼
        self.grammar_patterns = {
            # åŠ©è¯é”™è¯¯
            "auxiliary_errors": [
                (r'çš„å¾ˆ', 'å¾ˆ'),  # "çš„å¾ˆå¥½" -> "å¾ˆå¥½"
                (r'çš„éå¸¸', 'éå¸¸'),
                (r'çš„ç‰¹åˆ«', 'ç‰¹åˆ«'),
                (r'çš„è¶…çº§', 'è¶…çº§')
            ],
            
            # é‡å¤è¯ä¿®æ­£
            "repetition_errors": [
                (r'(çœŸçš„)çœŸçš„', r'\1'),  # "çœŸçš„çœŸçš„" -> "çœŸçš„"
                (r'(ç‰¹åˆ«)ç‰¹åˆ«', r'\1'),
                (r'(éå¸¸)éå¸¸', r'\1'),
                (r'(è¶…çº§)è¶…çº§', r'\1')
            ],
            
            # è¯åºé”™è¯¯
            "word_order_errors": [
                (r'å¥½å¾ˆ', 'å¾ˆå¥½'),  # "å¥½å¾ˆ" -> "å¾ˆå¥½"
                (r'ç¾å¾ˆ', 'å¾ˆç¾'),
                (r'æ£’å¾ˆ', 'å¾ˆæ£’'),
                (r'å¥½çœ‹å¾ˆ', 'å¾ˆå¥½çœ‹')
            ],
            
            # ä»‹è¯é”™è¯¯
            "preposition_errors": [
                (r'åœ¨è¿™ä¸ª', 'è¿™ä¸ª'),  # "åœ¨è¿™ä¸ªé¢œè‰²" -> "è¿™ä¸ªé¢œè‰²"
                (r'å¯¹è¿™ä¸ª', 'è¿™ä¸ª'),
                (r'çš„è¿™ä¸ª', 'è¿™ä¸ª')
            ]
        }
    
    def correct_grammar(self, text: str) -> Tuple[str, List[Dict[str, Any]]]:
        """ä¿®æ­£è¯­æ³•é”™è¯¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            (ä¿®æ­£åæ–‡æœ¬, åº”ç”¨çš„ä¿®æ­£åˆ—è¡¨)
        """
        try:
            corrected_text = text
            applied_corrections = []
            
            # åº”ç”¨å„ç±»è¯­æ³•ä¿®æ­£
            for error_type, patterns in self.grammar_patterns.items():
                for pattern, replacement in patterns:
                    matches = list(re.finditer(pattern, corrected_text))
                    for match in reversed(matches):  # ä»åå‘å‰æ›¿æ¢é¿å…ä½ç½®åç§»
                        original = match.group()
                        new_text = re.sub(pattern, replacement, corrected_text)
                        
                        if new_text != corrected_text:
                            correction = {
                                "type": f"grammar_{error_type}",
                                "original": original,
                                "corrected": replacement,
                                "confidence": 0.7,
                                "position": match.start()
                            }
                            applied_corrections.append(correction)
                            corrected_text = new_text
            
            return corrected_text, applied_corrections
            
        except Exception as e:
            self.logger.error(f"è¯­æ³•é”™è¯¯ä¿®æ­£å¤±è´¥: {e}")
            return text, []

class PunctuationOptimizer:
    """æ ‡ç‚¹ç¬¦å·ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def optimize_punctuation(self, text: str) -> Tuple[str, List[Dict[str, Any]]]:
        """ä¼˜åŒ–æ ‡ç‚¹ç¬¦å·
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            (ä¼˜åŒ–åæ–‡æœ¬, åº”ç”¨çš„ä¼˜åŒ–åˆ—è¡¨)
        """
        try:
            optimized_text = text
            applied_optimizations = []
            
            # 1. æ¸…ç†å¤šä½™ç©ºæ ¼
            original_text = optimized_text
            optimized_text = re.sub(r'\s+', ' ', optimized_text).strip()
            if optimized_text != original_text:
                applied_optimizations.append({
                    "type": "space_normalization",
                    "description": "æ¸…ç†å¤šä½™ç©ºæ ¼",
                    "confidence": 0.9
                })
            
            # 2. æ ‡ç‚¹ç¬¦å·è§„èŒƒåŒ–
            punct_rules = [
                (r'[,ï¼Œ]{2,}', 'ï¼Œ'),        # å¤šä¸ªé€—å·
                (r'[.ã€‚]{2,}', 'ã€‚'),        # å¤šä¸ªå¥å·
                (r'[!ï¼]{2,}', 'ï¼'),        # å¤šä¸ªæ„Ÿå¹å·
                (r'[?ï¼Ÿ]{2,}', 'ï¼Ÿ'),        # å¤šä¸ªé—®å·
                (r'[â€¦]{2,}', 'â€¦'),          # å¤šä¸ªçœç•¥å·
            ]
            
            for pattern, replacement in punct_rules:
                if re.search(pattern, optimized_text):
                    optimized_text = re.sub(pattern, replacement, optimized_text)
                    applied_optimizations.append({
                        "type": "punctuation_normalization",
                        "pattern": pattern,
                        "replacement": replacement,
                        "confidence": 0.8
                    })
            
            # 3. æ·»åŠ ç¼ºå¤±çš„å¥å·
            if optimized_text and not re.search(r'[ã€‚ï¼ï¼Ÿ]$', optimized_text):
                # æ ¹æ®å†…å®¹ç±»å‹å†³å®šæ ‡ç‚¹
                if any(word in optimized_text for word in ['å“‡', 'å¤ª', 'ç»äº†', 'çˆ±äº†']):
                    optimized_text += 'ï¼'  # æ„Ÿå¹å·
                    punct_type = 'æ„Ÿå¹å·'
                else:
                    optimized_text += 'ã€‚'  # å¥å·
                    punct_type = 'å¥å·'
                
                applied_optimizations.append({
                    "type": "punctuation_addition",
                    "added": punct_type,
                    "confidence": 0.6
                })
            
            return optimized_text, applied_optimizations
            
        except Exception as e:
            self.logger.error(f"æ ‡ç‚¹ç¬¦å·ä¼˜åŒ–å¤±è´¥: {e}")
            return text, []

class LiveStreamTextPostProcessor:
    """ç›´æ’­æ–‡æœ¬åå¤„ç†å™¨
    
    æ•´åˆæƒ…æ„Ÿè¡¨è¾¾çº é”™ã€äº§å“åç§°æ ‡å‡†åŒ–ã€ç½‘ç»œç”¨è¯­è§„èŒƒåŒ–ã€è¯­æ³•ä¿®æ­£å’Œæ ‡ç‚¹ä¼˜åŒ–
    """
    
    def __init__(self):
        # åˆå§‹åŒ–å„ä¸ªå¤„ç†å™¨
        self.emotion_corrector = EmotionExpressionCorrector()
        self.product_standardizer = ProductNameStandardizer()
        self.slang_normalizer = SlangNormalizer()
        self.grammar_corrector = GrammarCorrector()
        self.punctuation_optimizer = PunctuationOptimizer()
        
        # ç»Ÿè®¡æ•°æ®
        self.processing_stats = {
            "total_processed": 0,
            "correction_types": defaultdict(int),
            "average_corrections_per_text": 0.0,
            "processing_confidence": 0.0
        }
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("ç›´æ’­æ–‡æœ¬åå¤„ç†å™¨å·²åˆå§‹åŒ–")
    
    def process_text(self, 
                    text: str,
                    emotion_type: Optional[EmotionType] = None,
                    confidence_threshold: float = MIN_CONFIDENCE_FOR_CORRECTION,
                    processing_types: Optional[List[ProcessingType]] = None) -> PostProcessingResult:
        """å¤„ç†æ–‡æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            emotion_type: æƒ…æ„Ÿç±»å‹æç¤º
            confidence_threshold: å¤„ç†ç½®ä¿¡åº¦é˜ˆå€¼
            processing_types: æŒ‡å®šçš„å¤„ç†ç±»å‹
            
        Returns:
            åå¤„ç†ç»“æœ
        """
        try:
            original_text = text
            processed_text = text
            all_corrections = []
            applied_types = []
            
            # é»˜è®¤å¤„ç†ç±»å‹
            if processing_types is None:
                processing_types = [
                    ProcessingType.EMOTION_CORRECTION,
                    ProcessingType.PRODUCT_STANDARDIZATION,
                    ProcessingType.SLANG_NORMALIZATION
                ]
            
            # 1. æƒ…æ„Ÿè¡¨è¾¾çº é”™
            if ProcessingType.EMOTION_CORRECTION in processing_types:
                processed_text, emotion_corrections = self.emotion_corrector.correct_emotion_expressions(
                    processed_text, emotion_type
                )
                if emotion_corrections:
                    all_corrections.extend(emotion_corrections)
                    applied_types.append(ProcessingType.EMOTION_CORRECTION)
            
            # 2. äº§å“åç§°æ ‡å‡†åŒ–
            if ProcessingType.PRODUCT_STANDARDIZATION in processing_types:
                processed_text, product_corrections = self.product_standardizer.standardize_product_names(
                    processed_text
                )
                if product_corrections:
                    all_corrections.extend(product_corrections)
                    applied_types.append(ProcessingType.PRODUCT_STANDARDIZATION)
            
            # 3. ç½‘ç»œç”¨è¯­è§„èŒƒåŒ–
            if ProcessingType.SLANG_NORMALIZATION in processing_types:
                processed_text, slang_corrections = self.slang_normalizer.normalize_slang(
                    processed_text
                )
                if slang_corrections:
                    all_corrections.extend(slang_corrections)
                    applied_types.append(ProcessingType.SLANG_NORMALIZATION)
            
            # 4. è¯­æ³•é”™è¯¯ä¿®æ­£
            processed_text, grammar_corrections = self.grammar_corrector.correct_grammar(
                processed_text
            )
            if grammar_corrections:
                all_corrections.extend(grammar_corrections)
            
            # 5. æ ‡ç‚¹ç¬¦å·ä¼˜åŒ–
            processed_text, punct_corrections = self.punctuation_optimizer.optimize_punctuation(
                processed_text
            )
            if punct_corrections:
                all_corrections.extend(punct_corrections)
            
            # 6. è®¡ç®—å¤„ç†ç½®ä¿¡åº¦
            processing_confidence = self._calculate_processing_confidence(
                all_corrections, len(original_text)
            )
            
            # 7. è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„ä¿®æ­£
            filtered_corrections = [
                corr for corr in all_corrections 
                if corr.get('confidence', 0) >= confidence_threshold
            ]
            
            # å¦‚æœç½®ä¿¡åº¦å¤ªä½ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬
            if processing_confidence < confidence_threshold:
                processed_text = original_text
                filtered_corrections = []
                applied_types = []
            
            # 8. æ›´æ–°ç»Ÿè®¡æ•°æ®
            self._update_processing_stats(filtered_corrections, processing_confidence)
            
            return PostProcessingResult(
                original_text=original_text,
                processed_text=processed_text,
                applied_corrections=filtered_corrections,
                processing_confidence=processing_confidence,
                processing_types=applied_types
            )
            
        except Exception as e:
            self.logger.error(f"æ–‡æœ¬åå¤„ç†å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›åŸå§‹æ–‡æœ¬
            return PostProcessingResult(
                original_text=text,
                processed_text=text,
                applied_corrections=[],
                processing_confidence=0.0,
                processing_types=[]
            )
    
    def _calculate_processing_confidence(self, corrections: List[Dict[str, Any]], 
                                       text_length: int) -> float:
        """è®¡ç®—å¤„ç†ç½®ä¿¡åº¦"""
        if not corrections:
            return 0.8  # æ— ä¿®æ­£æ—¶çš„é»˜è®¤ç½®ä¿¡åº¦
        
        # åŸºäºä¿®æ­£ç½®ä¿¡åº¦çš„åŠ æƒå¹³å‡
        total_confidence = sum(corr.get('confidence', 0.5) for corr in corrections)
        avg_confidence = total_confidence / len(corrections)
        
        # è€ƒè™‘ä¿®æ­£æ•°é‡å¯¹ç½®ä¿¡åº¦çš„å½±å“
        correction_ratio = len(corrections) / max(text_length / 5, 1)  # å¹³åœ‡5ä¸ªå­—ä¸€ä¸ªä¿®æ­£
        
        if correction_ratio > 0.5:  # ä¿®æ­£è¿‡å¤šå¯èƒ½é™ä½ç½®ä¿¡åº¦
            confidence_penalty = min(correction_ratio - 0.5, 0.3)
            avg_confidence -= confidence_penalty
        
        return max(0.0, min(avg_confidence, 1.0))
    
    def _update_processing_stats(self, corrections: List[Dict[str, Any]], 
                               confidence: float):
        """æ›´æ–°å¤„ç†ç»Ÿè®¡æ•°æ®"""
        self.processing_stats["total_processed"] += 1
        
        # ç»Ÿè®¡ä¿®æ­£ç±»å‹
        for correction in corrections:
            correction_type = correction.get('type', 'unknown')
            self.processing_stats["correction_types"][correction_type] += 1
        
        # æ›´æ–°å¹³å‡å€¼
        count = self.processing_stats["total_processed"]
        prev_avg_corrections = self.processing_stats["average_corrections_per_text"]
        prev_avg_confidence = self.processing_stats["processing_confidence"]
        
        self.processing_stats["average_corrections_per_text"] = (
            (prev_avg_corrections * (count - 1) + len(corrections)) / count
        )
        self.processing_stats["processing_confidence"] = (
            (prev_avg_confidence * (count - 1) + confidence) / count
        )
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = dict(self.processing_stats)
        stats["correction_types"] = dict(stats["correction_types"])
        return stats
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡æ•°æ®"""
        self.processing_stats = {
            "total_processed": 0,
            "correction_types": defaultdict(int),
            "average_corrections_per_text": 0.0,
            "processing_confidence": 0.0
        }
        self.logger.info("åå¤„ç†ç»Ÿè®¡æ•°æ®å·²é‡ç½®")

# å¯¼å‡ºä¸»è¦ç±»
__all__ = [
    'LiveStreamTextPostProcessor',
    'EmotionExpressionCorrector',
    'ProductNameStandardizer',
    'SlangNormalizer',
    'GrammarCorrector',
    'PunctuationOptimizer',
    'PostProcessingResult',
    'CorrectionRule'
]